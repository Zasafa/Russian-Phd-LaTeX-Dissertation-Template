\chapter{Метод стахостической оптимизации при решении обратной задачи
  теории Ми} \label{chapt2}

\section{Сравнение методов стахостической оптимизации}
\label{sec:construct-review}

Развитие нанофотоники, как и многих других разделов физики, происходит
при одновременном развитии теории и эксперимента, которые оказываются
тесно связаны между собой.  Эксперимент определяет меру правильности
теоретических построений и, в то же время, является источником новых
физических эффектов. В последнем случае, определяющей становится роль
теоретического анализа, необходимого для выделения сути явления из
вороха возможных побочных факторов.  Аналогично, при исследовании
оптических свойств наночастиц можно брать результаты эксперимента и
проверять, насколько они соотносятся с существующими моделями, либо
вначале теоретически находить дизайны с новыми физическими свойствами,
а уже потом проверять их экспериментально.  Учитывая высокую стоимость
и трудоёмкость эксперимента в области оптики наночастиц вариант с
предварительным теоретическим анализом проблемы оказывается более
привлекательным.

Теория Ми позволяет изучать целый ряд физических величин, среди
которых стоит отметить, например, сечение рассеяния, сечение
поглощения и распределение электромагнитного поля как внутри, так и
вблизи наночастицы.  Дополнительно в теории Ми есть возможность
разделять вклады связанные с дипольными резонансами, квадрупольными и
мультиполями большего порядка. Такое разнообразие сильно затрудняет
решение обратной задачи, где требуется определить дизайн наночастицы
для достижения заданных характеристик взаимодействия с падающей
волной. Кроме того, заданным параметрам взаимодействия падающей волны
и наночастицы может соответствовать несколько дизайнов или же ни
одного.

Попытка решать эту задачу полностью аналитическми сразу же приводит к
необходимости выбора тех характеристик взаимодействия поля с частицей,
чьи значения будут определять наиболее подходящий дизайн. Другими
словами, необходимо будет решать обратную задачу отдельно для сечения
рассеяния, отдельно для коэффициента усиления поля и так далее, а это
многократно увеличивает объём небходимых работ. Более того,
использование только аналитического подхода заведомо оказывается
неприменимым при учёте экспериментально измеренных дисперсионных
зависимостей материальных параметров. Например, когда требуется
определить дизайн частицы из заданных металлов для получения нужных
рабочих характеристик в какой-то полосе частот и дополнительно
определить оптимальную длину волны внутри выбранного диапазона.  В
результате, наиболее универсальным представляется численное решение,
когда компьютерная программа выбирает параметры дизайна наночастицы
максимально приближенные к оптимальным.

Для численного решения обратную задачу теории Ми можно
переформулировать в общем виде. Будем рассматривать расчёт по теории
Ми в виде некой сложной функции, которая в контексте численного
решения называется целевой функций $f(\boldsymbol{x})$, в англоязычной литературе
используются теримины objective funtion или fitness function.  Целевая
функция получает вектор значений для входных параметров, который в
нашем случае представляет из себя список материальных параметров и
толщин для каждого слоя. Результат вычисления целевой функции
получается в виде скаляра, который при решении задачи Ми может быть
как значением некой физической величины, получаемой из расчёта, так и
некой искусственной величиной, характеризующей отклик системы в целом,
например, отношение сечений рассеяния и поглощения. Тогда обратную
задачу можно сформулировать, как поиск такого вектора входных
параметров для целевой функции, который позволял бы получить на выходе
результате значение, равное заранее заданной величине или ниболее
приближенное к ней. Таким образом, на этапе формулирования общего вида
задачи снимается вопрос о существовании её решения: если нет такого
входного вектора параметров, из которого можно было точно получить
заданное значение на выходе, то будет получено приблизительного
решение. Этого может оказаться достаточно для обеспечения потребностей
широкого круга практических задач.

При такой постановке вопроса становится возможным использовать
различные методы численной оптимизации, которые позволяют находить
положение эстремума у произвольной функции. Поиск вектора входных
параметров $\boldsymbol{x}$, который позволяет получить целевое
значение $y_t=f(\boldsymbol{x})$, сводится к поиску минимума для новой
целевой функции $\left|f(\boldsymbol{x})-y_t\right|$. 

При выборе конкретного метода оптимизации возникает сложность,
обусловленная огромным количеством методов и большим числом
модификаций каждого метода. При выборе метода применительно к задаче
Ми были использованны следующие предпосылки:
\begin{itemize}
\item Несмотря на то, что решение Ми является аналитическим и
  выражается в виде разложения в ряд по сферическим векторным
  гармоникам, одновременное нахождение производных для зависимости от
  радиуса и материального параметра оказывается громоздким даже в
  случае однородной сферы, что тем более верно для случая
  произвольного числа сферических слоёв.  В связи с чем метод
  оптимизации не должен требовать для своей работы нахождения значений
  производных оптимизируемой функции.  Это особено актуально в случае,
  когда одновременно оптимизируются и толщина, и показатель
  преломления каждого слоя.
\item Решение образовано быстро-осциллирующими функциями и, как
  следствие, будет содержать большое количество локальных
  экстремумов. Таким образом, алгоритмы оптимизации, требующие особого
  отношения к подобным случаям, оказываются заведомо менее
  производительными.
\item Параметры оптимизации и оптимизируемая величина являются
  вещественными числами.
\end{itemize}

Всё вместе это приводит к необходимости исключить из рассмотрения
такие популярные методы, как метод наискорейшего спуска (требующий
вычисления градиента), симплекс-метод Нелдера--Мида (есть сложность с
локальными экстремумами) и аналогичные им. В результате приходится
ограничить выбор стохастическими методами, среди которых наиболее
распространёнными являются генетические
алгоритмы~\cite{Goldberg-GA-1989}, методы роя
частиц~\cite{Kennedy-PSO-1995} и методы дифференциальной
эволюции~\cite{Storn-DE-first-1997}.  Все эти алгоритмы используют
метод <<проб и ошибок>>.  Несколько пробных решений (называемых
индивидами) генерируются случайным образом и многократно улучшаются в
надежде найти некое удовлетворительное решение. Качество решения
оценивается целевой функцией, формулируемой в задаче, которую
предстоит оптимизировать.  Полная группа индивидов называется
популяцией.  Состояние популяции на конкретном шаге итерации
называется поколением.  Переход между поколениями осуществляется в
соответствии с рядом относительно простых правил, которые составляют
сущность определённого алгоритма.

Генетические алгоритмы обычно рассматривают вещественные числа в виде
набора битов.  В отличие от них, методы роя частиц и методы
дифференциальной эволюции могут работать в непрерывном пространстве
вещественных входных параметров естественным образом (используя
возможность сложения и вычитания векторов пробных решений), что делает
их гораздо более удобными для решения физических и инженерных
задач.  Производительность этих алгоритмов зависит от правильного
выбора значений некоторых внутренних параметров
алгоритма.  Использование адаптивных версий алгоритмов упрощает задачу
оптимизации: их значения внутренних параметров настраиваются
автоматически при переходе между поколениями. Как правило, адаптивным
алгоритмам нужно гораздо меньше (более чем на порядок) итераций, чем
неадаптивным, чтобы добиться того же результата оптимизации.

Сравнение~\cite{Gong-compare-EA-2014,Kang-compare-EA-RABC-2011}
адаптивного алгоритма дифференциальной эволюции
JADE~\cite{Jingqiao-JADE-2009} с адаптивной оптимизацией методом роя
частиц~\cite{Zhan-APSO-2008} и многими другими адаптиными методами
эволюционной оптимизации показало превосходство JADE или результат
сопоставимый с лучшими из числа протестированных методов оптимизации
для большинства стандартных
тестов~\cite{Schwefel-1981,Rosenbrock-1960,Muhlenbein-1991,back-1996,Griewank-1981}.
Такое преимущество и отсносительная простота алогритма JADE послужили
основанием для того, чтобы выбрать его в качестве основного алгоритма
оптимизации в настоящей работе. Дополнительно для алгоритма JADE была
реализована улучшенная скорость скрещивания (по алгоритму
PMCRADE~\cite{Li-PMCRADE-2011}), получившийся метод оптимизации был
применён в последующих главах диссертации.

\section{Реализация алгоритма JADE в виде программы.}
\label{sec:jade}

Часть технических моментов, касающихся выбора языка программирования и
вопросов производительности итоговой программы, была изложена в
разделе~\ref{sec:code} и остаются верными и для реализации алгоритма
стохастической оптимизации.


Выполненная в рамках настоящей работы реализация указанного метода
позволяет эффективно использовать современные процессоры с большим
количеством параллельных потоков вычисления и может выполняться на
суперкомпьютерных кластерах, что стало возможно благодаря
использованию программных библиотек, поддерживающих стандарт Message
Parsing Interface (MPI).  Разработанное~\cite{JADE-web} программное
обеспечение успешно проходит набор стандартных тестов для алгоритмов
оптимизации.

\begingroup % Ограничиваем область видимости arraystretch
\renewcommand{\arraystretch}{1.6}%% Увеличение расстояния между рядами, для улучшения восприятия.
\begin{longtabu} to \textwidth {@{}>{\setlength{\baselineskip}{0.7\baselineskip}}X[1.1mc]>{\setlength{\baselineskip}{0.7\baselineskip}}X[mc]X[4]@{}}
        \caption{Тестовые функции для оптимизации, $D$ -
          размерность. Для всех функций значение в точке глобального
          минимума равно нулю.\label{tbl:test-functions}}\\% label всегда желательно идти после caption 
        
        \toprule     %%% верхняя линейка
        Имя           &Стартовый диапазон параметров &Функция  \\ 
        \midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5 
        \endfirsthead

        \multicolumn{3}{c}{\small\slshape (продолжение)}        \\ 
        \toprule     %%% верхняя линейка
        Имя           &Стартовый диапазон параметров &Функция  \\ 
        \midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5 
        \endhead
        
        \multicolumn{3}{c}{\small\slshape (окончание)}        \\ 
        \toprule     %%% верхняя линейка
        Имя           &Стартовый диапазон параметров &Функция  \\ 
        \midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5 
        \endlasthead

        \bottomrule %%% нижняя линейка
        \multicolumn{3}{r}{\small\slshape продолжение следует}  \\ 
        \endfoot   
        \endlastfoot

        сфера         &$\left[-100,\,100\right]^D$   &
        $\begin{aligned}\textstyle f_1(\boldsymbol{x})=\sum_{i=1}^Dx_i^2\end{aligned}$                                                        \\
        Schwefel 2.22 &$\left[-10,\,10\right]^D$     &
        $\begin{aligned}\textstyle f_2(\boldsymbol{x})=\sum_{i=1}^D|x_i|+\prod_{i=1}^D|x_i|\end{aligned}$                                     \\
        Schwefel 1.2  &$\left[-100,\,100\right]^D$   &$\begin{aligned}\textstyle f_3(\boldsymbol{x})=\sum_{i=1}^D\left(\sum_{j=1}^ix_j\right)^2\end{aligned}$                               \\
        Schwefel 2.21 &$\left[-100,\,100\right]^D$   &$\begin{aligned}\textstyle f_4(\boldsymbol{x})={\rm max}_i\!\left\{\left|x_i\right|\right\}\end{aligned}$                             \\
        Rosenbrock    &$\left[-30,\,30\right]^D$     &$\begin{aligned}\textstyle f_5(\boldsymbol{x})=\sum_{i=1}^{D-1}\left[100\!\left(x_{i+1}-x_i^2\right)^2+(x_i-1)^2\right]\end{aligned}$ \\
        ступенчатая   &$\left[-100,\,100\right]^D$   &$\begin{aligned}\textstyle f_6(\boldsymbol{x})=\sum_{i=1}^D\big\lfloor x_i+0.5\big\rfloor^2\end{aligned}$                             \\ 
зашумлённая квартическая  &$\left[-1.28,\,1.28\right]^D$ &$\begin{aligned}\textstyle f_7(\boldsymbol{x})=\sum_{i=1}^Dix_i^4+rand[0,1)\end{aligned}$\vspace*{2ex}\\
        Schwefel 2.26 &$\left[-500,\,500\right]^D$   &$\begin{aligned}f_8(\boldsymbol{x})= &\textstyle\sum_{i=1}^D-x_i\,\sin\sqrt{|x_i|}\,+ \\
                    &\vphantom{\sum}+ D\cdot
                    418.98288727243369 \end{aligned}$\\
        Rastrigin     &$\left[-5.12,\,5.12\right]^D$ &
        $\begin{aligned}\textstyle
          f_9(\boldsymbol{x})=\sum_{i=1}^D\left[x_i^2-10\,\cos(2\pi
            x_i)+10\right]\end{aligned}$\vspace*{2ex}\\
  Ackley        &$\left[-32,\,32\right]^D$     &$\begin{aligned}f_{10}(\boldsymbol{x})= &\textstyle -20\, {\rm exp}\!\left(-0.2\sqrt{\frac{1}{D}\sum_{i=1}^Dx_i^2} \right)-\\
                    &\textstyle - {\rm exp}\left(\frac{1}{D}\sum_{i=1}^D\cos(2\pi x_i)  \right)  + 20 + e \end{aligned}$ \\
        Griewank      &$\left[-600,\,600\right]^D$
        &$\begin{aligned}f_{11}(\boldsymbol{x})= &\textstyle \frac{1}{4000}
          \sum_{i=1}^{D}x_i^2 - \prod_{i=1}^D\cos\left(x_i/\sqrt{i}\right) +1     \end{aligned}$ \vspace*{3ex} \\
        штрафная 1    &$\left[-50,\,50\right]^D$     &
        $\begin{aligned}f_{12}(\boldsymbol{x})= &\textstyle \frac{\pi}{D}
          \Big\{ 10\,\sin^2(\pi y_1) +\\ &+
          \textstyle \sum_{i=1}^{D-1}(y_i-1)^2\left[1+10\,\sin^2(\pi
              y_{i+1})\right] +\\ &+(y_D-1)^2 \Big\} +\textstyle\sum_{i=1}^D u(x_i,\,10,\,100,\,4)            \end{aligned}$ \vspace*{2ex} \\
        штрафная 2    &$\left[-50,\,50\right]^D$     &
        $\begin{aligned}f_{13}(\boldsymbol{x})= &\textstyle 0.1
          \Big\{\sin^2(3\pi x_1) +\\ &+
          \textstyle \sum_{i=1}^{D-1}(x_i-1)^2\left[1+\sin^2(3 \pi
              x_{i+1})\right] + \\ &+(x_D-1)^2\left[1+\sin^2(2\pi
              x_D)\right] \Big\} +\\ &+\textstyle\sum_{i=1}^D u(x_i,\,5,\,100,\,4)            \end{aligned}$            \vspace*{1ex}\\
        \midrule%%% тонкий разделитель
        \multicolumn{3}{@{}p{\textwidth}}{%
            \vspace*{-4ex}% этим подтягиваем повыше
            \hspace*{2.5em}% абзацный отступ - требование ГОСТ 2.105
            Примечание "---  Для функций $f_{12}$ и $f_{13}$
            используется $y_i = 1 + \frac{1}{4}(x_i+1)$ и
            $u(x_i,\,a,\,k,\,m)=\begin{cases}
k(x_i-a)^m,\quad &x_i >a\\[-0.5em]
0,\quad &-a\leq x_i \leq a\\[-0.5em]
k(-x_i-a)^m,\quad &x_i <-a
\end{cases}$  }   \\        \bottomrule %%% нижняя линейка 
\end{longtabu} 
\endgroup




\begingroup % Ограничиваем область видимости arraystretch
\renewcommand{\arraystretch}{1.6}%% Увеличение расстояния между рядами, для улучшения восприятия.
\begin{longtabu} to \textwidth {@{}>{\setlength{\baselineskip}{0.7\baselineskip}}X[1.1mc]>{\setlength{\baselineskip}{0.7\baselineskip}}X[mc]X[4]@{}}
        \caption{Сравнение различных алгоритмов оптимизации.\label{tbl:opt-results-book}}\\% label всегда желательно идти после caption 
        
        \toprule     %%% верхняя линейка
        Имя           &Стартовый диапазон параметров &Функция  \\ 
        \midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5 
        \endfirsthead

        \multicolumn{3}{c}{\small\slshape (продолжение)}        \\ 
        \toprule     %%% верхняя линейка
        Имя           &Стартовый диапазон параметров &Функция  \\ 
        \midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5 
        \endhead
        
        \multicolumn{3}{c}{\small\slshape (окончание)}        \\ 
        \toprule     %%% верхняя линейка
        Имя           &Стартовый диапазон параметров &Функция  \\ 
        \midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5 
        \endlasthead

        \bottomrule %%% нижняя линейка
        \multicolumn{3}{r}{\small\slshape продолжение следует}  \\ 
        \endfoot   
        \endlastfoot

        сфера         &$\left[-100,\,100\right]^D$   &
        $\begin{aligned}\textstyle f_1(\boldsymbol{x})=\sum_{i=1}^Dx_i^2\end{aligned}$                                                        \\
        Schwefel 2.22 &$\left[-10,\,10\right]^D$     &
        $\begin{aligned}\textstyle f_2(\boldsymbol{x})=\sum_{i=1}^D|x_i|+\prod_{i=1}^D|x_i|\end{aligned}$                                     \\
        Schwefel 1.2  &$\left[-100,\,100\right]^D$   &$\begin{aligned}\textstyle f_3(\boldsymbol{x})=\sum_{i=1}^D\left(\sum_{j=1}^ix_j\right)^2\end{aligned}$                               \\
        Schwefel 2.21 &$\left[-100,\,100\right]^D$   &$\begin{aligned}\textstyle f_4(\boldsymbol{x})={\rm max}_i\!\left\{\left|x_i\right|\right\}\end{aligned}$                             \\
        Rosenbrock    &$\left[-30,\,30\right]^D$     &$\begin{aligned}\textstyle f_5(\boldsymbol{x})=\sum_{i=1}^{D-1}\left[100\!\left(x_{i+1}-x_i^2\right)^2+(x_i-1)^2\right]\end{aligned}$ \\
        ступенчатая   &$\left[-100,\,100\right]^D$   &$\begin{aligned}\textstyle f_6(\boldsymbol{x})=\sum_{i=1}^D\big\lfloor x_i+0.5\big\rfloor^2\end{aligned}$                             \\ 
зашумлённая квартическая  &$\left[-1.28,\,1.28\right]^D$ &$\begin{aligned}\textstyle f_7(\boldsymbol{x})=\sum_{i=1}^Dix_i^4+rand[0,1)\end{aligned}$\vspace*{2ex}\\
        Schwefel 2.26 &$\left[-500,\,500\right]^D$   &$\begin{aligned}f_8(\boldsymbol{x})= &\textstyle\sum_{i=1}^D-x_i\,\sin\sqrt{|x_i|}\,+ \\
                    &\vphantom{\sum}+ D\cdot
                    418.98288727243369 \end{aligned}$\\
        Rastrigin     &$\left[-5.12,\,5.12\right]^D$ &
        $\begin{aligned}\textstyle
          f_9(\boldsymbol{x})=\sum_{i=1}^D\left[x_i^2-10\,\cos(2\pi
            x_i)+10\right]\end{aligned}$\vspace*{2ex}\\
  Ackley        &$\left[-32,\,32\right]^D$     &$\begin{aligned}f_{10}(\boldsymbol{x})= &\textstyle -20\, {\rm exp}\!\left(-0.2\sqrt{\frac{1}{D}\sum_{i=1}^Dx_i^2} \right)-\\
                    &\textstyle - {\rm exp}\left(\frac{1}{D}\sum_{i=1}^D\cos(2\pi x_i)  \right)  + 20 + e \end{aligned}$ \\
        Griewank      &$\left[-600,\,600\right]^D$
        &$\begin{aligned}f_{11}(\boldsymbol{x})= &\textstyle \frac{1}{4000}
          \sum_{i=1}^{D}x_i^2 - \prod_{i=1}^D\cos\left(x_i/\sqrt{i}\right) +1     \end{aligned}$ \vspace*{3ex} \\
        штрафная 1    &$\left[-50,\,50\right]^D$     &
        $\begin{aligned}f_{12}(\boldsymbol{x})= &\textstyle \frac{\pi}{D}
          \Big\{ 10\,\sin^2(\pi y_1) +\\ &+
          \textstyle \sum_{i=1}^{D-1}(y_i-1)^2\left[1+10\,\sin^2(\pi
              y_{i+1})\right] +\\ &+(y_D-1)^2 \Big\} +\textstyle\sum_{i=1}^D u(x_i,\,10,\,100,\,4)            \end{aligned}$ \vspace*{2ex} \\
        штрафная 2    &$\left[-50,\,50\right]^D$     &
        $\begin{aligned}f_{13}(\boldsymbol{x})= &\textstyle 0.1
          \Big\{\sin^2(3\pi x_1) +\\ &+
          \textstyle \sum_{i=1}^{D-1}(x_i-1)^2\left[1+\sin^2(3 \pi
              x_{i+1})\right] + \\ &+(x_D-1)^2\left[1+\sin^2(2\pi
              x_D)\right] \Big\} +\\ &+\textstyle\sum_{i=1}^D u(x_i,\,5,\,100,\,4)            \end{aligned}$            \vspace*{1ex}\\
        \midrule%%% тонкий разделитель
        \multicolumn{3}{@{}p{\textwidth}}{%
            \vspace*{-4ex}% этим подтягиваем повыше
            \hspace*{2.5em}% абзацный отступ - требование ГОСТ 2.105
            Примечание "---  Для функций $f_{12}$ и $f_{13}$
            используется $y_i = 1 + \frac{1}{4}(x_i+1)$ и
            $u(x_i,\,a,\,k,\,m)=\begin{cases}
k(x_i-a)^m,\quad &x_i >a\\[-0.5em]
0,\quad &-a\leq x_i \leq a\\[-0.5em]
k(-x_i-a)^m,\quad &x_i <-a
\end{cases}$  }   \\        \bottomrule %%% нижняя линейка 
\end{longtabu} 

TODO рассказать про разные генераторы случайных чисел.

 Было получено
свидетельство о государственной регистрации программы для
ЭВМ~№2014611568.
\clearpage